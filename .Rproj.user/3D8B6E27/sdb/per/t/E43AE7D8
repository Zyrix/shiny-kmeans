{
    "contents" : "---\ntitle       : K-means Clustering\nsubtitle    : A Shiny App Illustration\nauthor      : Renaud Dufour\njob         : \nframework   : io2012        # {io2012, html5slides, shower, dzslides, ...}\nhighlighter : highlight.js  # {highlight.js, prettify, highlight}\nhitheme     : tomorrow      # \nwidgets     : []            # {mathjax, quiz, bootstrap}\nmode        : selfcontained # {standalone, draft}\nknit        : slidify::knit2slides\n---\n\n## What is k-means clustering ?\n\n* K-means is a distance-based method for cluster analysis in data mining\n* It enables partitioning a set of data points into groups which are as similar as possible\n* Each group, called cluster, is represented by its center\n\n# Algorithm  \n  \nGiven K, the number of clusters, k-means clustering works as follows:\n\n* Select K points as initial centroids\n* __Repeat__\n    * Form K clusters by assigning each point to its closest centroid\n    * Re-compute the centroids of each cluster\n* __Until__ convergence criterion is satisfied\n* Different kinds of measures can be used (L1 norm, L2 norm, cosine similarity, ...)\n\n--- .class #id \n\n## The Shiny App\n\n* Illustrates K-mean clustering based on 2 datasets:\n    * the R built in __iris__ dataset\n    * a dataset __dat1__ involving embedded clusters\n* Enables to change the following parameters:\n    * dataset to be used\n    * variables on which the clustering is to be performed (note: 2D clustering only)\n    * number of clusters\n    * type of kernel : linear or radial (RBF)\n* When using a non-linear kernel, the datapoints are first projected into the kernel spaced before clustering is performed.  \n\nThe Application can be accessed directly [here](https://duf59.shinyapps.io/shiny-kmeans/)\n\n--- .class #id \n\n## Example\n\n* __Left panel__ : iris dataset, variables _sepal.length_ and _sepal.width_, 3 clusters and linear kernel\n* __Right panel__ : dat1 dataset, variables _x_ and _y_, 2 clusters and RBF kernel. There we see that using a radial kernel enables distinguishing the two embedded clusters\n\n```{r echo=FALSE, message=FALSE, results='hide',fig.width=12, fig.height=5, fig.align='center'}\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\ndata(iris)\ndata <- iris[,1:2]\nK <- 2\ncenters <- data[1:K,]\n\nKclust <- kmeans(data ,data.matrix(centers), iter.max = 100)\nresult <- data.frame(data, cluster=as.factor(Kclust$cluster))\n\ng1 <- ggplot(data=result, aes(x=Sepal.Length, y=Sepal.Width, color = cluster)) +\n    geom_point(size=3) + \n    geom_point(data=as.data.frame(Kclust$centers),\n               aes(x=Sepal.Length, y=Sepal.Width, color='Center'),\n               pch=17, size=7) +\n    xlab(names(data)[1]) + ylab(names(data)[2])\n\nlibrary(kernlab)\n\ndata   <- read.table(\"dat1/self_test.data\", skip=1, col.names = c(\"x\",\"y\"))\n\nK <- 2  # number of cluster \ncenters <- data[1:K,]\n\nKclust <- kkmeans(as.matrix(data), centers=2, kernel=\"rbfdot\")\nresult <- data.frame(data, cluster=as.factor(Kclust@.Data))\n\ng2 <- ggplot(data=result, aes(x=x, y=y, color=cluster)) +\n    geom_point(size=3) + \n    geom_point(data=as.data.frame(Kclust@centers),\n               aes(x=V1, y=V2, color='Center'),\n               pch=17, size=7) +\n    xlab(\"x\") + ylab(\"y\")\n\ngrid.arrange(g1,g2,ncol=2)\n\n```\n\n\n--- .class #id \n\n## Source Code and further improvements\n\n* The source code of the shiny App is available on my [GitHub Repo](https://github.com/duf59/shiny-kmeans)\n* More informations on the K-means algorithm on [wikipedia](http://en.wikipedia.org/wiki/K-means_clustering). I also recommend the [Cluster Analysis In Data Mining](https://www.coursera.org/course/clusteranalysis) class on Coursera, which actually inspired me this app.\n* Some possible improvements of this app include :\n    * using interactive graphics (rchart, googleVis)\n    * computing clustering validation measures such as purity, normalized mutual information, ... Note that such external measures require knowing the true classes of the data points, which is the case for the 2 implemented datasets but not in general. Instead one could also consider internal measures such as Beta CV.\n    * Implementing other kernels (actually kkmeans() from the kernel packahe is used to compute kmeans with RBF kernel. Other kernels built in in kkmeans() could be easily added as an input)\n    * Allow user to tune kernel parameters (actually the sigma parameter of RBF kernel is internally determined using an heuristic approach)\n\n\n\n",
    "created" : 1431863112359.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3542874221",
    "id" : "E43AE7D8",
    "lastKnownWriteTime" : 1431871290,
    "path" : "C:/Users/Renaud_2/Dropbox/Coursera/Data Science Specialization/08_Developping data product/shiny-kmeans/presentation/index.Rmd",
    "project_path" : "presentation/index.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}